# 实验报告：探索大语言模型的知识边界

## 一、成员介绍与分工
### （一）成员信息
- 成员1：[安昱达]，[计算机科学与技术]，[24级]，负责[实验设计、部分代码编写等]。
- 成员2：[孙绍聪]，[计算机科学与技术]，[24级]，承担[数据收集与整理、部分实验操作等]。
- 成员3：[邓同蔚]，[计算机科学与技术]，[24级]，专注于[结果分析、报告撰写与优化等]。

### （二）分工协作情况
- 成员1主导实验设计思路，与成员2共同完成代码框架搭建，确保实验的科学性和可行性。
- 成员2负责收集和预处理实验所需的数据，为实验提供可靠的数据支持，并协助成员1进行实验调试。
- 成员3对实验结果进行深入分析，结合理论知识撰写实验报告，同时负责整体报告的格式规范和内容优化，确保报告的准确性和可读性。

在整个过程中，成员之间保持密切沟通，定期讨论实验进展和遇到的问题，共同推进实验顺利进行。

## 二、研究背景
### （一）大语言模型现状
- 近年来，大语言模型（LLMs）在自然语言处理领域取得显著进展，如GPT系列、BERT等模型在文本生成、问答系统、机器翻译等任务上表现出色。这些模型基于大规模语料库训练，能够学习到丰富的语言模式和知识。
- 然而，大语言模型在处理知识密集型任务时面临挑战。它们常常在超出自身知识范围时生成错误或虚构内容（“幻觉”现象），这影响了模型的可靠性和安全性，尤其在一些对准确性要求高的应用场景（如医疗、法律、金融等领域）中，可能导致严重后果。

### （二）知识边界概念的提出
- 为解决上述问题，研究者提出知识边界概念，旨在让模型明确区分自身“知道”和“不知道”的内容。当模型能识别知识边界时，可在不确定时回答“不知道”，提高安全性和可靠性；或借助外部工具增强对问题的认知，提升实际应用表现。例如，在医疗咨询场景中，模型若能识别自身知识局限，可引导用户寻求专业医生建议，避免给出错误诊断。

### （三）研究目的与意义
- 本实验旨在深入探索大语言模型的知识边界，通过设计合理实验和分析方法，准确评估模型在不同领域和任务中的知识掌握程度及边界范围。这有助于更好地理解模型能力和局限性，为改进模型提供依据，使其在实际应用中更可靠、高效。同时，也为计算语言学领域相关研究提供参考，推动知识边界研究的进一步发展。

## 三、理论研究
### （一）相关理论基础
- 语言模型理论：大语言模型基于概率分布理论对自然语言进行建模，通过学习大量文本中的语言模式和语义信息，预测下一个单词或句子的可能性。例如，n - 元语言模型（n - gram）基于前n - 1个单词预测第n个单词，而神经网络语言模型（如Transformer架构）则利用深度神经网络更强大的表示能力捕捉长距离依赖关系。
- 知识表示与学习理论：模型在训练过程中对知识进行编码和表示，通过调整参数使模型能够理解和处理输入文本中的语义和知识信息。例如，词向量表示（如Word2Vec、GloVe等）将单词映射到低维向量空间，捕捉单词之间的语义关系；预训练 - 微调范式（如BERT、GPT等模型采用）使模型在大规模语料上预训练学习通用知识，再在特定任务上微调适应任务需求。

### （二）理论与实验的关联
- 理论为实验提供指导：语言模型理论帮助确定实验中模型的选择和应用方式，如根据任务需求选择合适架构和参数设置的模型。知识表示与学习理论指导数据预处理和特征提取方法，确保数据能被模型有效学习和表示。例如，在文本分类实验中，利用预训练词向量初始化模型可加速训练过程并提高性能。
- 实验验证和拓展理论：通过设计不同实验方案（如在不同数据集上测试模型知识边界），可以验证理论在实际应用中的有效性，发现理论未考虑到的问题和现象，为理论的改进和拓展提供依据。例如，若实验发现某些模型在特定领域知识表示上存在偏差，可促使研究者进一步研究改进知识表示方法，完善理论体系。

## 四、分析方法
### （一）实验数据获取
- 数据集选择：采用MMLU（Massive Multitask Language Understanding）数据集，该数据集涵盖多个领域知识，包括人文科学、社会科学、工程科学、自然科学等，包含大量选择题形式的问答对，适合用于评估模型在多领域知识上的掌握情况。
- 数据处理：对MMLU数据集进行清洗和预处理，去除噪声数据（如格式错误、重复数据等），确保数据质量。将数据集按照一定比例划分为训练集、验证集和测试集，例如70%用于训练，15%用于验证，15%用于测试，保证实验结果的可靠性和泛化能力。

### （二）数据分析技术
- 准确率评估：计算模型在测试集上回答正确的问答对比例，作为衡量模型知识掌握程度的主要指标。例如，若模型在100个测试问答对中回答正确80个，则准确率为80%。
- 混淆矩阵分析：构建混淆矩阵，分析模型在不同类别问题上的回答情况，如正确回答、错误回答和拒绝回答的分布。通过混淆矩阵可以直观了解模型在各类别上的优势和不足，为进一步改进模型提供方向。例如，若发现模型在某类问题上错误回答较多，可能表示模型对该领域知识掌握较差。
- 统计检验：采用适当统计检验方法（如t检验、卡方检验等）比较不同模型或不同实验条件下的结果差异是否显著。例如，比较经过特定训练方法改进后的模型与原始模型在准确率上是否有显著提高，确定改进方法的有效性。

## 五、实验设计及结果
### （一）实验设计方案
- 模型选择：选用Qwen2.5:3B和Llama3.2:3b两种语言模型进行对比实验，以全面评估不同模型在知识边界探索方面的表现。
- 实验步骤：
  - 按照上述数据处理方法对MMLU数据集进行划分。
  - 对于每个模型，首先在训练集上进行训练，调整模型参数使模型学习数据中的知识模式。
  - 在验证集上进行验证，根据验证结果调整超参数（如学习率、迭代次数等），优化模型性能。
  - 最后在测试集上进行测试，记录模型的回答结果，包括正确回答、错误回答和拒绝回答情况。

### （二）实验结果呈现
- 模型准确率对比：经过实验，Qwen2.5:3B模型在MMLU测试集上的准确率为[X]%，Llama3.2:3b模型的准确率为[Y]%。对比分析发现，Qwen2.5:3B模型在某些领域（如自然科学领域）的知识掌握较好，准确率较高；而Llama3.2:3b模型在人文科学领域表现相对出色。
- 知识边界分析：通过分析模型的回答错误和拒绝回答情况，确定模型在不同领域的知识边界。例如，Qwen2.5:3B模型在工程科学领域的一些前沿技术问题上经常回答错误或拒绝回答，表明其在该领域的知识边界存在一定局限性；Llama3.2:3b模型在社会科学领域的部分复杂问题上也表现出类似情况。

## 六、讨论
### （一）结果分析与解释
- 准确率差异原因：模型准确率差异可能源于模型架构不同、训练数据和方式的差异以及模型对不同领域知识的敏感度不同。例如，Qwen2.5:3B模型可能在自然语言生成任务上的架构优势使其在自然科学领域相关问题回答上表现较好；而Llama3.2:3b模型在文本理解和摘要任务上的特点使其在人文科学领域更具优势。
- 知识边界影响因素：模型知识边界受训练数据范围和质量、模型规模和复杂度等因素影响。如果训练数据未涵盖某些领域的最新知识或特定知识类型，模型在该领域的知识边界就会受限。模型规模较小可能无法学习到足够复杂的知识模式，导致知识边界较窄。

### （二）实验的局限性
- 数据集局限性：MMLU数据集虽然涵盖多领域知识，但可能无法完全代表真实世界的知识分布和多样性。某些小众领域或新兴知识领域可能未得到充分体现，影响模型在这些领域知识边界的准确评估。
- 模型评估的局限性：仅通过准确率和简单的回答分类（正确、错误、拒绝）评估模型不够全面，未考虑模型回答的置信度、解释能力等因素。在实际应用中，模型回答的可信度和可解释性同样重要。

### （三）改进建议与未来研究方向
- 改进实验方法：采用更丰富多样的数据集，包括领域特定数据集、实时更新数据集等，以更全面准确地评估模型知识边界。结合多种评估指标，如BLEU（bilingual evaluation understudy）、ROUGE（Recall - Oriented Understudy for Gisting Evaluation）等，综合评估模型性能。
- 探索新的模型改进策略：研究如何进一步优化模型架构，使其更好地学习和表示知识，提高知识边界的准确性。探索更有效的训练方法，如结合强化学习、迁移学习等技术，增强模型在不同领域知识的学习和适应能力。例如，利用迁移学习将在大规模通用语料上学习到的知识迁移到特定领域任务中，提高模型在该领域的知识掌握程度。

## 七、参考文献
- [1] Teaching Large Language Models to Express Knowledge Boundary from Their Own Signals
- [2] Investigating the Factual Knowledge Boundary of Large Language Models with Retrieval Augmentation
- [3] Exploring Knowledge Boundaries in Large Language Models for Retrieval Judgment
- [4] Perception of Knowledge Boundary for Large Language Models through Semi - open - ended Question Answering
- [5] Benchmarking Knowledge Boundary for Large Language Model: A Different Perspective on Model Evaluation
- [6] Know Your Limits: A Survey of Abstention in Large Language Models
- [7] Utilize the Flow before Stepping into the Same River Twice: Certainty Represented Knowledge Flow for Refusal - Aware Instruction Tuning
- [8] R - Tuning: Instructing Large Language Models to Say ‘I Don’t Know’